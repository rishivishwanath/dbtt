# DBT (Data Processing & Analysis of IPL Matches)

## Project Overview
This project implements a comprehensive data processing and analysis system for IPL cricket matches using Apache Kafka and Spark. It features both real-time streaming and batch processing capabilities, along with performance comparison and visualization components.

## Architecture
```
                    [Kafka Topics]
                    ├── deliveries_raw
[Input Data] ──►    ├── runs          ──► [Spark Streaming] ──► [MySQL] ──► [Batch Processing] ──► [Visualization]
                    ├── rate
                    └── bowler
```

## Folder Structure
```
project/
├── analysis/
│   └── performance_comparison.py    # Compares streaming vs batch performance
├── batch/
│   └── batch_processor.py          # Batch processing implementation
├── data/
│   └── ipl_2022_deliveries.csv     # Sample match data
├── kafka/
│   ├── producer.py                 # Produces messages to Kafka topics
│   ├── db_consumer.py             # Stores Kafka messages to MySQL
│   └── test_consumer.py           # Test consumer implementation
├── spark/
│   ├── bowl.py                    # Bowling analysis
│   ├── highest_score_overall.py   # Overall highest scores
│   ├── highest_score_team_match.py# Team-wise highest scores
│   └── runrate.py                 # Run rate analysis
├── visualization/
│   └── visualizer.py              # Data visualization implementation
└── requirements.txt               # Project dependencies
```

## Prerequisites
- Python 3.8+
- Apache Kafka 2.8+
- Apache Spark 3.0+
- MySQL 8.0+
- Required Python libraries (see requirements.txt)

## Setup Instructions

### 1. Environment Setup
1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd DBT
   ```

2. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### 2. MySQL Setup
1. Create the database:
   ```bash
   mysql -u root -p
   ```
   ```sql
   CREATE DATABASE ipl_analytics;
   ```

### 3. Kafka Setup
1. Start Zookeeper:
   ```bash
   zookeeper-server-start.bat C:\kafka\config\zookeeper.properties
   ```

2. Start Kafka Server:
   ```bash
   kafka-server-start.bat C:\kafka\config\server.properties
   ```

3. Create required topics:
   ```bash
   kafka-topics.bat --create --topic deliveries_raw --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
   kafka-topics.bat --create --topic runs --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
   kafka-topics.bat --create --topic rate --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
   kafka-topics.bat --create --topic bowler --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
   ```

## Implementation Steps

### 1. Start Data Storage Consumer
```bash
python kafka/db_consumer.py
```

### 2. Launch Spark Streaming Jobs
Run each in a separate terminal:
```bash
spark-submit spark/bowl.py
spark-submit spark/runrate.py
spark-submit spark/highest_score_overall.py
spark-submit spark/highest_score_team_match.py
```

### 3. Start Data Producer
```bash
python kafka/producer.py
```

### 4. Run Performance Analysis
```bash
python analysis/performance_comparison.py
```

### 5. Generate Visualizations
```bash
python visualization/visualizer.py
```

## Key Features

### Streaming Analysis
- Real-time ball-by-ball processing
- Live run rate calculations
- Instant bowling performance metrics
- Dynamic highest score tracking

### Batch Processing
- Comprehensive player statistics
- Team performance analysis
- Match-wise analytics
- Historical trend analysis

### Performance Comparison
- Execution time comparison
- Accuracy metrics
- Data consistency validation
- Detailed performance reports

### Visualization
- Top batsmen performance
- Bowling statistics
- Team comparisons
- Win percentages

## Monitoring and Maintenance

### Logs
- Kafka logs: Check C:\kafka\logs
- Spark logs: Available in spark-submit console output
- Application logs: Generated by individual components

### Common Issues and Solutions
1. Kafka Connection Issues:
   - Verify Zookeeper is running
   - Ensure Kafka server is active
   - Check topic existence

2. Spark Job Failures:
   - Verify Spark installation
   - Check Java environment variables
   - Review executor memory settings

3. Database Connectivity:
   - Confirm MySQL service status
   - Verify credentials
   - Check network connectivity

## Contributing
1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Create Pull Request

## License
This project is licensed under the MIT License - see the LICENSE file for details.
